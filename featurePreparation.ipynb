{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset before the selection:\n",
    "* 13778028 $QCD$ events \n",
    "* 26335315 $tt$ events\n",
    "* 11083533 $W+jets$ events\n",
    "\n",
    "Total of 51196876 events, \n",
    "<br>\n",
    "Dataset after the selection: \n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1) Features preparation\n",
    "\n",
    "**HLF classifier**\n",
    "* Convert HLF in a vector dense type (Needed to use the MinMax scaler)\n",
    "    * New column `hfeatures_dense`\n",
    "* Create a small pipeline with the following steps (for HLF):\n",
    "    * One-hot-encode\n",
    "        * New column `encoded_label`\n",
    "    * MinMax scaler \n",
    "        * New column `HLF_input`\n",
    "    \n",
    "**Particle-sequence classifier**\n",
    "* For each event:\n",
    "    * Sort particles by decreasing $\\Delta R$ distance from the isolated lepton\n",
    "    * Filter the empty rows\n",
    "    * Scale the features\n",
    "* Create the new column `GRU_input`\n",
    "\n",
    "### 2)  Dataset preparation\n",
    "\n",
    "* Shuffle the dtaframe\n",
    "* Create test and train dataframes (fraction: 0.8/0.2 (train/test))\n",
    "    * `~12M` for training and `4M` for test \n",
    "* Create train/test dataset of different sizes and save them as parquet files\n",
    "    * `100k`/`20k`\n",
    "    * `1M`/`200k`\n",
    "    * `6M`/ `1M`\n",
    "    * `12M` / `4M` (full dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/spark/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_name = 'FeaturePreparation'\n",
    "master = \"yarn\"\n",
    "num_executors = 60\n",
    "executor_memory = '6G'\n",
    "driver_memory = '128G'\n",
    "num_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "\n",
    "os.environ[\"PYTHONHOME\"] = \"/afs/cern.ch/work/m/migliori/public/anaconda2\"\n",
    "os.environ[\"PYTHONPATH\"] = \"/afs/cern.ch/work/m/migliori/public/anaconda2/lib/python2.7/site-packages\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(application_name)\\\n",
    "        .config(\"spark.pyspark.python\",\n",
    "                \"/afs/cern.ch/work/m/migliori/public/anaconda2/bin/python\")\\\n",
    "        .config(\"spark.master\", master)\\\n",
    "        .config(\"spark.executor.cores\", `num_cores`)\\\n",
    "        .config(\"spark.executor.instances\", `num_executors`)\\\n",
    "        .config(\"spark.executor.memory\", executor_memory)\\\n",
    "        .config(\"spark.driver.memory\", driver_memory)\\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"128G\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", 'false')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ithdp1063.cern.ch:5200\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FeaturePreparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd6706dab50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20021476 events\n",
      "CPU times: user 8.12 ms, sys: 10 ms, total: 18.1 ms\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.format(\"parquet\") \\\n",
    "        .load(\"hdfs://analytix/cms/bigdatasci/vkhriste/data/events2features_19092018\")\n",
    "\n",
    "events = data.count()\n",
    "print('There are', events, 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many events there are foreach class. The three classes are labeled as follows:\n",
    "\n",
    "* $QCD=0$\n",
    "* $t\\bar{t}=1$\n",
    "* $W=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 ms, sys: 10.2 ms, total: 15.6 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = data.groupBy('label').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:\n",
      "\t* 1426343 QCD events (frac = 0.071)\n",
      "\t* 14265397 tt events (frac = 0.713)\n",
      "\t* 4329736 W+jets events (frac = 0.216)\n"
     ]
    }
   ],
   "source": [
    "labels = ['QCD', 'tt', 'W+jets']\n",
    "\n",
    "qcd_events = 0\n",
    "tt_events = 0 \n",
    "wjets_events = 0\n",
    "\n",
    "print('There are:')\n",
    "for i in range(3):\n",
    "    print('\\t*',counts[i][1],labels[counts[i].label],\n",
    "          'events (frac = {:.3f})'.format(counts[i][1]*1.0/events))\n",
    "    if counts[i].label==0:\n",
    "        qcd_events = counts[i][1]\n",
    "    elif counts[i].label==1:\n",
    "        tt_events = counts[i][1] \n",
    "    elif counts[i].label==2:\n",
    "        wjets_events = counts[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preparation \n",
    "\n",
    "Elements of the `hfeatures` column are list, hence we need to convert them into `Vectors.Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "vector_dense_udf = udf(lambda r : Vectors.dense(r),VectorUDT())\n",
    "data = data.withColumn('hfeatures_dense',vector_dense_udf('hfeatures'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the pipeline to scale HLF and encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 294 ms, sys: 293 ms, total: 587 ms\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "## One-Hot-Encode\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"label\"],\n",
    "                                 outputCols=[\"encoded_label\"],\n",
    "                                 dropLast=False)\n",
    "\n",
    "## Scale feature vector\n",
    "scaler = MinMaxScaler(inputCol=\"hfeatures_dense\",\n",
    "                      outputCol=\"HLF_input\")\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, scaler])\n",
    "\n",
    "%time fitted_pipeline = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = fitted_pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the particle-sequence classifier, we need to sort the particles in each event by decreasing $\\Delta R$ distance from the isolated lepton, where \n",
    "\n",
    "$$\n",
    "\\Delta R = \\sqrt{\\Delta \\eta^2 + \\Delta \\phi^2}\n",
    "$$\n",
    "\n",
    "From the production of low level we know that the isolated lepton is the first particle and the $19$ features (foreach particle) are: <br>\n",
    "\n",
    "['Energy', 'Px', 'Py', 'Pz', 'Pt', 'Eta', 'Phi', 'vtxX', 'vtxY', 'vtxZ', 'ChPFIso', 'GammaPFIso', 'NeuPFIso', 'isChHad', 'isNeuHad', 'isGamma', 'isEle', 'isMu', 'Charge'] <br>\n",
    "\n",
    "\n",
    "hence we need feature $5$ ($\\eta$) and $6$ ($\\phi$) to compute $\\Delta R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class lepAngularCoordinates():\n",
    "    \n",
    "    def __init__(self, eta, phi):\n",
    "        self.Eta = eta\n",
    "        self.Phi = phi\n",
    "        \n",
    "    def Phi_mpi_pi(self, x):\n",
    "        while x >= math.pi:\n",
    "            x -= 2*math.pi\n",
    "        while x < math.pi:\n",
    "            x += 2*math.pi\n",
    "        return x\n",
    "    \n",
    "    def DeltaR(self, eta, phi):\n",
    "        deta = self.Eta - eta\n",
    "        dphi = self.Phi_mpi_pi(self.Phi - phi)\n",
    "        return math.sqrt(deta*deta + dphi*dphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import numpy as np\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(DoubleType())))\n",
    "def transform(particles):\n",
    "    ## The isolated lepton is the first partiche in the list\n",
    "    ISOlep = lepAngularCoordinates(particles[0][5], particles[0][6])\n",
    "    \n",
    "    ## Sort the particles based on the distance from the isolated lepton\n",
    "    particles.sort(key = lambda part: ISOlep.DeltaR(part[5], part[6]),\n",
    "                   reverse=True)\n",
    "    \n",
    "    ## Filter the empty rows before standardizing \n",
    "    #particles = np.asarray(particles)\n",
    "    #particles = particles[~(particles==0.).all(axis=1)]\n",
    "    \n",
    "    ## Standardize\n",
    "    particles = StandardScaler().fit_transform(particles).tolist()\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('GRU_input', transform('lfeatures'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hfeatures_dense: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  Balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd = data.filter('label=0')\n",
    "tt = data.filter('label=1')\n",
    "wjets = data.filter('label=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the undersampled dataframes\n",
    "tt = tt.sample(False, qcd_events*1.0/tt_events) \n",
    "wjets = wjets.sample(False, qcd_events*1.0/wjets_events)\n",
    "\n",
    "dataUndersampled = qcd.union(tt).union(wjets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    0|1426343|\n",
      "|    1|1427480|\n",
      "|    2|1425283|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check if the undersample worked\n",
    "dataUndersampled.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "trainUndersampled, testUndersampled = dataUndersampled.randomSplit([0.8, 0.2], seed=42)\n",
    "trainUndersampled = trainUndersampled.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 s, sys: 2.85 s, total: 6.17 s\n",
      "Wall time: 1h 55min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## write to parquet\n",
    "PATH = 'hdfs://analytix/user/migliori/HLT/'\n",
    "trainUndersampled.write.parquet(PATH+'trainUndersampled_v2.parquet',\n",
    "                                              mode='overwrite')\n",
    "testUndersampled.write.parquet(PATH+'testUndersampled_v2.parquet',\n",
    "                                              mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Balance tt/wjets\n",
    "\n",
    "Create the same number of tt and W events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd = data.filter('label=0')\n",
    "tt = data.filter('label=1')\n",
    "wjets = data.filter('label=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tt.sample(False, wjets_events*1.0/tt_events)\n",
    "dataUndersampled_tt = qcd.union(tt).union(wjets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    0|1426343|\n",
      "|    1|4327752|\n",
      "|    2|4329736|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataUndersampled_tt.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "trainUndersampled_tt, testUndersampled_tt = dataUndersampled_tt.randomSplit([0.9, 0.1], seed=42)\n",
    "trainUndersampled_tt = trainUndersampled_tt.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.49 s, sys: 2.9 s, total: 6.39 s\n",
      "Wall time: 2h 20min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## write to parquet\n",
    "PATH = 'hdfs://analytix/user/migliori/HLT/'\n",
    "trainUndersampled_tt.write.parquet(PATH+'trainUndersampled_tt_v2.parquet',\n",
    "                                              mode='overwrite')\n",
    "testUndersampled_tt.write.parquet(PATH+'testUndersampled_tt_v2.parquet',\n",
    "                                              mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Save the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "train, test = data.randomSplit([0.9, 0.1], seed=42)\n",
    "train = train.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## write to parquet\n",
    "PATH = 'hdfs://analytix/user/migliori/HLT/'\n",
    "train.write.parquet(PATH+'train.parquet', mode='overwrite')\n",
    "test.write.parquet(PATH+'test.parquet',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
