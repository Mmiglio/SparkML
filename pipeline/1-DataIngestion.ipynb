{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Ingestion** is the first stage of the pipeline. Here we will read the ROOT file from HDFS into a Spark dataframe using [Spark-ROOT](https://github.com/diana-hep/spark-root) reader and then we will create the Low Level Features (LLF) and High Level Features datasets.\n",
    "\n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: LCG 94 (it has spark 2.3.1)\n",
    "* *Platform*: centos7-gcc7\n",
    "* *Spark cluster*: Hadalytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://swan005.cern.ch:45042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataIngestion_swan</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efe2ad86250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Spark Session has been created correctly\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a file containing functions that we will use later\n",
    "spark.sparkContext.addPyFile(\"utilFunctions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from HDFS\n",
    "<br>\n",
    "As first step we will read the samples into a Spark dataframe using Spark-Root. We will select only a subset of columns present in the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"hdfs://hadalytic/project/ML/data/data_root/\"\n",
    "\n",
    "samples = [\"qcd\", \"ttbar\", \"wjets\"]\n",
    "\n",
    "requiredColumns = [\n",
    "    \"EFlowTrack\",\n",
    "    \"EFlowNeutralHadron\",\n",
    "    \"EFlowPhoton\",\n",
    "    \"Electron\",\n",
    "    \"MuonTight\",\n",
    "    \"MuonTight_size\",\n",
    "    \"Electron_size\",\n",
    "    \"MissingET\",\n",
    "    \"Jet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind qcd sample\n",
      "Loadind ttbar sample\n",
      "Loadind wjets sample\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "dfList = []\n",
    "\n",
    "for label,sample in enumerate(samples):\n",
    "    print(\"Loadind {} sample\".format(sample))\n",
    "    tmpDF = spark.read \\\n",
    "                .format(\"org.dianahep.sparkroot.experimental\") \\\n",
    "                .load(PATH + sample + \"*.root\") \\\n",
    "                .select(requiredColumns) \\\n",
    "                .withColumn(\"label\", lit(label))\n",
    "    dfList.append(tmpDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all samples into a single dataframe\n",
    "df = dfList[0]\n",
    "for tmpDF in dfList[1:]:\n",
    "    df = df.union(tmpDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataframe can fit in memory it is possible to cache it to ensure faster access to data. To do this just add the \n",
    "```Python\n",
    "df = df.cache()\n",
    "```\n",
    "and in the next action Spark will cache it. Be aware that this might take some time.\n",
    "\n",
    "Let's have a look at how many events there are for each class. Keep in mind that the labels are mapped as follow\n",
    "* $0=\\text{QCD}$\n",
    "* $1=\\text{t}\\bar{\\text{t}}$\n",
    "* $2=\\text{W}+\\text{jets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|79996|\n",
      "|    2|90000|\n",
      "|    0|35101|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of events per sample\n",
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of events\n",
    "events = df.count()\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EFlowTrack: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- fUniqueID: integer (nullable = true)\n",
      " |    |    |-- fBits: integer (nullable = true)\n",
      " |    |    |-- PID: integer (nullable = true)\n",
      " |    |    |-- Charge: integer (nullable = true)\n",
      " |    |    |-- PT: float (nullable = true)\n",
      " |    |    |-- Eta: float (nullable = true)\n",
      " |    |    |-- Phi: float (nullable = true)\n",
      " |    |    |-- EtaOuter: float (nullable = true)\n",
      " |    |    |-- PhiOuter: float (nullable = true)\n",
      " |    |    |-- X: float (nullable = true)\n",
      " |    |    |-- Y: float (nullable = true)\n",
      " |    |    |-- Z: float (nullable = true)\n",
      " |    |    |-- T: float (nullable = true)\n",
      " |    |    |-- XOuter: float (nullable = true)\n",
      " |    |    |-- YOuter: float (nullable = true)\n",
      " |    |    |-- ZOuter: float (nullable = true)\n",
      " |    |    |-- TOuter: float (nullable = true)\n",
      " |    |    |-- Dxy: float (nullable = true)\n",
      " |    |    |-- SDxy: float (nullable = true)\n",
      " |    |    |-- Xd: float (nullable = true)\n",
      " |    |    |-- Yd: float (nullable = true)\n",
      " |    |    |-- Zd: float (nullable = true)\n",
      " |    |    |-- EFlowTrack_Particle: struct (nullable = true)\n",
      " |    |    |    |-- TObject: struct (nullable = true)\n",
      " |    |    |    |    |-- fUniqueID: integer (nullable = true)\n",
      " |    |    |    |    |-- fBits: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's print the schema of one of the required columns\n",
    "df.select(\"EFlowTrack\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we are dealing with highly nested data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create derivate datasets\n",
    "\n",
    "Now we will create the LLF and HLF datasets. This is done by the function `convert` below which takes as input an event (i.e. the list of particles present in that event) and do the following steps:\n",
    "1. Select the events with at least one isolated electron/muon (implemented in `selection`)\n",
    "2. Create the list of 801 particles and the 19 low level features for each of them\n",
    "3. Compute the high level features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lorentz Vector and other functions for pTmaps\n",
    "from utilFunctions import *\n",
    "\n",
    "def selection(event, TrkPtMap, NeuPtMap, PhotonPtMap, PTcut=23., ISOcut=0.45):\n",
    "    \"\"\"\n",
    "    This function simulates the trigger selection. \n",
    "    Foreach event the presence of one isolated muon or electron with pT >23 GeV is required\n",
    "    \"\"\"\n",
    "    if event.Electron_size == 0 and event.MuonTight_size == 0: \n",
    "        return False, False, False\n",
    "    \n",
    "    foundMuon = None \n",
    "    foundEle =  None \n",
    "    \n",
    "    l = LorentzVector()\n",
    "    \n",
    "    for ele in event.Electron:\n",
    "        if ele.PT <= PTcut: continue\n",
    "        l.SetPtEtaPhiM(ele.PT, ele.Eta, ele.Phi, 0.)\n",
    "        \n",
    "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
    "        if foundEle == None and (pfisoCh+pfisoNeu+pfisoGamma)<ISOcut:\n",
    "            foundEle = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
    "                        0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                        0., 0., 0., 1., 0., float(ele.Charge)]\n",
    "    \n",
    "    for muon in event.MuonTight:\n",
    "        if muon.PT <= PTcut: continue\n",
    "        l.SetPtEtaPhiM(muon.PT, muon.Eta, muon.Phi, 0.)\n",
    "        \n",
    "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
    "        if foundMuon == None and (pfisoCh+pfisoNeu+pfisoGamma)<ISOcut:\n",
    "            foundMuon = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
    "                         0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                         0., 0., 0., 0., 1., float(muon.Charge)]\n",
    "            \n",
    "    if foundEle != None and foundMuon != None:\n",
    "        if foundEle[5] > foundMuon[5]:\n",
    "            return True, foundEle, foundMuon\n",
    "        else:\n",
    "            return True, foundMuon, foundEle\n",
    "    if foundEle != None: return True, foundEle, foundMuon\n",
    "    if foundMuon != None: return True, foundMuon, foundEle\n",
    "    \n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def convert(event):\n",
    "    \"\"\"\n",
    "    This function takes as input an event, applies trigger selection \n",
    "    and create LLF and HLF datasets\n",
    "    \"\"\"\n",
    "    q = LorentzVector()\n",
    "    particles = []\n",
    "    TrkPtMap = ChPtMapp(0.3, event)\n",
    "    NeuPtMap = NeuPtMapp(0.3, event)\n",
    "    PhotonPtMap = PhotonPtMapp(0.3, event)\n",
    "    if TrkPtMap.shape[0] == 0: return Row()\n",
    "    if NeuPtMap.shape[0] == 0: return Row()\n",
    "    if PhotonPtMap.shape[0] == 0: return Row()\n",
    "    \n",
    "    #\n",
    "    # Get leptons\n",
    "    #\n",
    "    selected, lep, otherlep = selection(event, TrkPtMap, NeuPtMap, PhotonPtMap)\n",
    "    if not selected: return Row()\n",
    "    particles.append(lep)\n",
    "    lepMomentum = LorentzVector(lep[1], lep[2], lep[3], lep[0])\n",
    "    \n",
    "    #\n",
    "    # Select Tracks\n",
    "    #\n",
    "    nTrk = 0\n",
    "    for h in event.EFlowTrack:\n",
    "        if nTrk>=450: continue\n",
    "        if h.PT<=0.5: continue\n",
    "        q.SetPtEtaPhiM(h.PT, h.Eta, h.Phi, 0.)\n",
    "        if lepMomentum.DeltaR(q) > 0.0001:\n",
    "            pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "            pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "            pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "            particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                              h.PT, h.Eta, h.Phi, h.X, h.Y, h.Z,\n",
    "                              pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                              1., 0., 0., 0., 0., float(np.sign(h.PID))])\n",
    "            nTrk += 1\n",
    "    \n",
    "    #\n",
    "    # Select Photons\n",
    "    #\n",
    "    nPhoton = 0\n",
    "    for h in event.EFlowPhoton:\n",
    "        if nPhoton >= 150: continue\n",
    "        if h.ET <= 1.: continue\n",
    "        q.SetPtEtaPhiM(h.ET, h.Eta, h.Phi, 0.)\n",
    "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "        particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h.ET, h.Eta, h.Phi, 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 0., 1., 0., 0., 0.])\n",
    "        nPhoton += 1\n",
    "    \n",
    "    #\n",
    "    # Select Neutrals\n",
    "    #\n",
    "    nNeu = 0\n",
    "    for h in event.EFlowNeutralHadron:\n",
    "        if nNeu >= 200: continue\n",
    "        if h.ET <= 1.: continue\n",
    "        q.SetPtEtaPhiM(h.ET, h.Eta, h.Phi, 0.)\n",
    "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "        particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h.ET, h.Eta, h.Phi, 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 1., 0., 0., 0., 0.])\n",
    "        nNeu += 1\n",
    "        \n",
    "    for iTrk in range(nTrk, 450):\n",
    "        particles.append([0., 0., 0., 0., 0., 0., 0., 0.,0.,\n",
    "                          0.,0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for iPhoton in range(nPhoton, 150):\n",
    "        particles.append([0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for iNeu in range(nNeu, 200):\n",
    "        particles.append([0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "        \n",
    "    #\n",
    "    # High Level Features\n",
    "    #\n",
    "    myMET = event.MissingET[0]\n",
    "    MET = myMET.MET\n",
    "    phiMET = myMET.Phi\n",
    "    MT = 2.*MET*lepMomentum.Pt()*(1-math.cos(lepMomentum.Phi()-phiMET))\n",
    "    HT = 0.\n",
    "    nJets = 0.\n",
    "    nBjets = 0.\n",
    "    for jet in event.Jet:\n",
    "        if jet.PT > 30 and abs(jet.Eta)<2.6:\n",
    "            nJets += 1\n",
    "            HT += jet.PT\n",
    "            if jet.BTag>0: \n",
    "                nBjets += 1\n",
    "    LepPt = lep[4]\n",
    "    LepEta = lep[5]\n",
    "    LepPhi = lep[6]\n",
    "    LepIsoCh = lep[10]\n",
    "    LepIsoGamma = lep[11]\n",
    "    LepIsoNeu = lep[12]\n",
    "    LepCharge = lep[18]\n",
    "    LepIsEle = lep[16]\n",
    "    hlf = Vectors.dense([HT, MET, phiMET, MT, nJets, nBjets, LepPt, LepEta, LepPhi,\n",
    "           LepIsoCh, LepIsoGamma, LepIsoNeu, LepCharge, LepIsEle])\n",
    "    \n",
    "        \n",
    "    #\n",
    "    # return the Row of low level features and high level features\n",
    "    #\n",
    "    \n",
    "    return Row(lfeatures=particles, hfeatures=hlf, label=event.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally apply the function to all the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.rdd \\\n",
    "            .map(convert) \\\n",
    "            .filter(lambda row: len(row) > 0) \\\n",
    "            .toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 242 ms, sys: 107 ms, total: 348 ms\n",
      "Wall time: 13min\n"
     ]
    }
   ],
   "source": [
    "DATASETS_PATH = \"hdfs://hadalytic/project/ML/data/swan/\"\n",
    "%time features.write.parquet(DATASETS_PATH+\"datasets.parquet\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.cores",
     "value": "4"
    },
    {
     "name": "spark.executor.memory",
     "value": "8G"
    },
    {
     "name": "spark.executor.instances",
     "value": "10"
    },
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.jars.packages",
     "value": "org.diana-hep:spark-root_2.11:0.1.16"
    },
    {
     "name": "spark.app.name",
     "value": "DataIngestion"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
